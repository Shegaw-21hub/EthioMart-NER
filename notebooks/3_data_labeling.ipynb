{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Preview of messages for labeling:\n",
      "\n",
      "ğŸŸ¦ Sample 1:\n",
      "Tokens: ['One', 'Step', 'Hair', 'Dryer', 'Styler', 'áŠ¨áˆ­áˆ', 'áˆˆáˆ˜áˆµáˆ«á‰µ', 'áˆˆáˆ›áˆˆáˆµáˆˆáˆµ', 'áŠ¥áŠ•á‹²áˆáˆ', 'áˆˆáˆ›á‹µáˆ¨á‰…', 'á‹¨áˆšá‹«áŒˆáˆˆáŒáˆ', 'áˆˆáŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ•', 'á€áŒ‰áˆ­', 'á‰°áˆµáˆ›áˆš', 'á‹¨áˆ™á‰€á‰µ', 'áˆ˜á‰†áŒ£áŒ áˆªá‹«', 'áˆµáˆ‹áˆˆá‹', 'áˆˆáŠ áŒ á‰ƒá‰€áˆ', 'áˆá‰¹', 'á‹‹áŒ‹á¦', '1600', 'á‰¥áˆ­', 'áŠ á‹µáˆ«áˆ»', 'áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰…', 'á‰¢áˆ®', 'á‰.', 'S05S06', '0902660722', '0928460606', 'á‰ Telegram', 'áˆˆáˆ›á‹˜á‹', 'á‹­áŒ á‰€áˆ™', 'zemencallcenter', 'zemenexpressadmin', 'áˆˆá‰°áŒ¨áˆ›áˆª', 'áˆ›á‰¥áˆ«áˆªá‹«', 'á‹¨á‰´áˆŒáŒáˆ«áˆ', 'áŒˆáƒá‰½áŠ•']\n",
      "\n",
      "ğŸŸ¦ Sample 2:\n",
      "Tokens: ['áˆ›áˆµáˆáŠ•áŒ áˆªá‹«á‹áŠ•', 'á‰°áŒ­áŠá‹', 'áŠ áˆáŠ‘áŠ‘', 'á‹­áˆ˜á‹áŒˆá‰¡', 'á¤', '10', 'á‰…áŠ“áˆ½', 'á‹«áŒáŠ™', '!', '!', 'á‹¨á‰°áŠ•áŒ‹á‹°á‹°á‹áŠ•', 'áŠá‹°áˆ', 'á‹¨áˆšá‹«á‰€áŠ“', 'áˆ˜áˆ‹á‹', 'á‹­áŠ¸á‹áŠ“', '!', '!']\n",
      "\n",
      "ğŸŸ¦ Sample 3:\n",
      "Tokens: ['2in1', 'Portable', 'Dumpling', 'Making', 'Machine', 'á‹‹áŒ‹á¦', '650', 'á‰¥áˆ­', 'á‹áˆµáŠ•', 'ááˆ¬', 'áŠá‹', 'á‹«áˆˆá‹', 'áŠ á‹µáˆ«áˆ»', 'á‰.áˆ˜áŒˆáŠ“áŠ›', 'áˆ˜áˆ°áˆ¨á‰µ', 'á‹°á‹áˆ­', 'áˆáˆ', 'áˆáˆˆá‰°áŠ›', 'áá‰…', 'á‰¢áˆ®', 'á‰.', 'S05S06', 'á‰.á’á‹«áˆ³', 'áŒŠá‹®áˆ­áŒŠáˆµ', 'áŠ á‹°á‰£á‰£á‹­', 'áˆ«áˆ˜á‰µ_á‰³á‰¦áˆ­_áŠ¦á‹³_áˆ…áŠ•áƒ', '1áŠ›', 'áá‰…', 'áˆ±á‰…', 'á‰.', 'G1', '107', '0902660722', '0928460606', 'á’á‹«áˆ³', 'á‰…áˆ­áŠ•áŒ«á', '0960460606', 'mardashope', 'á‰ Telegram', 'áˆˆáˆ›á‹˜á‹', 'á‹­áŒ á‰€áˆ™', 'zemencallcenter', 'zemenexpressadmin', 'áˆˆá‰°áŒ¨áˆ›áˆª', 'áˆ›á‰¥áˆ«áˆªá‹«', 'á‹¨á‰´áˆŒáŒáˆ«áˆ', 'áŒˆáƒá‰½áŠ•']\n",
      "\n",
      "ğŸŸ¦ Sample 4:\n",
      "Tokens: ['Rubiks', 'Cubes', 'Sail', '6.0', '3', 'by', '3', 'áˆ©á‰¢áŠ­áˆµ', 'áŠªá‹©á‰¥', '33', 'áŠ áˆáŒáˆªá‹áˆáŠ•', 'á‰ áˆ˜áŒ á‰€áˆ', 'áŠ áˆ°áˆ«áˆ©áŠ•', 'á‹¨áˆšá‹«áˆ³á‹­', 'áˆ›áŠ‘á‹‹áˆ', 'á‹«áˆˆá‹', 'áŒ áŠ•áŠ«áˆ«áŠ“', 'á‰ ááŒ¥áŠá‰µ', 'áˆˆáˆ˜áˆ¥áˆ«á‰µ', 'áŠ áˆ˜á‰º', 'á‰ áŠ áˆˆáˆ›á‰½áŠ•', 'á‰°á‹ˆá‹³áŒ…', 'á‹¨áˆ†áŠ', 'áˆ˜áŒ«á‹ˆá‰»', 'áŠ¨áˆáŒ…', 'áŠ¥áˆµáŠ¨', 'áŠ á‹‹á‰‚', 'áˆáˆ‰áˆ', 'á‹¨áˆšá‹áŠ“áŠ“á‰ á‰µ', 'á‹¨áˆ›áˆ°áˆ‹áˆ°áˆ', 'á‰½áˆá‰³áŠ•áŠ“', 'á‹¨áˆ›áˆµá‰³á‹ˆáˆµ', 'áŠ á‰…áˆáŠ•', 'á‹¨áˆšá‹«á‹³á‰¥áˆ­', 'áŠ¨á‹µá‰¥áˆ­á‰µ', 'áˆˆáˆ˜áˆ‹á‰€á‰…', 'áŠ¥áŠ“', 'á‹¨áˆ˜áŒ¨áŠ“áŠá‰…', 'áˆµáˆœá‰µáŠ•', 'áˆˆáˆ˜á‰€áŠáˆµ', 'á‹¨áˆšáˆ¨á‹³', 'á‰ áˆ«áˆµ', 'áˆ˜á‰°áˆ›áˆ˜áŠ•áŠ•', 'á‹¨áˆšá‹«á‹³á‰¥áˆ­', 'á‰½áŒáˆ­áŠ•', 'á‹¨áˆ˜áá‰³á‰µ', 'áŠ­áˆ…áˆá‰µáŠ•', 'á‹¨áˆšáŒ¨áˆáˆ­', 'áˆˆáŠáŒˆáˆ®á‰½', 'á‰µáŠ©áˆ¨á‰µ', 'áˆ˜áˆµáŒ á‰µáŠ•', 'á‹¨áˆšáˆˆáˆ›áˆ˜á‹±á‰ á‰µ', 'á‰µá‹•áŒáˆµá‰µáŠ•', 'á‹¨áˆšáˆˆáˆ›áˆ˜á‹±á‰ á‰µ', 'á‹¨áŠ¥áŒ…áŠ“', 'á‹¨áŠ á‹­áŠ•', 'á‰…áŠ•áŒ…á‰µáŠ•', 'á‹¨áˆšá‹«á‹³á‰¥áˆ­', 'á‹‹áŒ‹', '600', 'áŠ á‹µáˆ«áˆ»á¦', 'áŒ‰áˆ­á‹µ', 'áˆ¾áˆ‹', 'áˆ†áˆŠ', 'áˆ²á‰²', 'áˆ´áŠ•á‰°áˆ­', '3áŠ›', 'áá‰…', 'á‹­áˆáŒ¡', 'á‰ áˆ˜áŠáˆ»á‹¬', 'á‹¨áˆáŒ…á‹áŠ•', 'áˆ˜áŠáˆ»', 'á‹«á‰…áˆáˆ‰', '!', 'á‹­á‹°á‹áˆ‰', '0989939393', 'á‹ˆá‹­áˆ', '0930323334', 'Telegram', 'Tiktok', 'Facebook', 'rubickscube', 'cube', 'confidence', 'concentration', 'creativity', 'gift', 'entertain', 'toy', 'kid', 'store', 'educationaltoys', 'meneshaye', 'meneshayestore']\n",
      "\n",
      "ğŸŸ¦ Sample 5:\n",
      "Tokens: ['HP', 'PAVILION', 'HP', 'PAVILION', 'X360', '14', 'FHD', 'TOUCH', 'SCREEN', 'CONVERTIBLE', 'CORE', 'I5', '12th', 'GEN', '16Gb', 'ğ——ğ——ğ—¥4', '512GB', 'SSD', 'INTEL', 'IRIS', 'XE', 'GRAPHICS', 'window', '10', 'pro', '6HR', 'brand', 'new', 'Price', 'rasneva', 'áˆˆáŠ áŒ­áˆ­', 'áˆ˜áˆáŠ¥áŠ­á‰µ', 'á‹­á‹°á‹‰áˆ‰', '251912759900', '251920153333', 'áŠ á‹µáˆ«áˆ»', 'áˆ˜áŒˆáŠ“áŠ›', 'áˆ›áˆ«á‰¶áŠ•', 'á‹¨', 'áŒˆá‰ á‹«', 'áˆ›áŠ¥áŠ¨áˆ', 'á‰ ', 'á‹‹áŠ“á‹', 'áˆ˜áŒá‰¢á‹«', 'áˆ˜áˆ¬á‰µ', 'áˆ‹á‹­', 'á‹ˆá‹­áŠ•áˆ', 'áŒáˆ«á‹áŠ•á‹µ', 'ááˆáˆ­', 'á‰¥á‰…', 'á‹­á‰ áˆ‰', 'áŠá‰«', 'áŠ®áˆá’á‹á‰°áˆ­', 'áˆ˜áˆ†áŠ‘áŠ•', 'á‹«áˆ¨áŒ‹áŒáŒ¡', 'á‹µáˆ…áˆ¨', 'áŒˆáŒ»á‰½áŠ•áŠ•', 'á‹­áŒá‰¥áŠ™', 'á‰´áˆŒáŒáˆ«áˆ', 'á‰»áŠ“áˆ‹á‰½áŠ•áŠ•', 'á‹­á‰€áˆ‹á‰€áˆ‰']\n",
      "\n",
      "âœ… Saved 2 labeled samples to: data/labeled/amharic_ner.conll\n",
      "\n",
      "ğŸ”· Sample 1/5\n",
      "\n",
      "ğŸŸ¨ Tokens to label: Hello world !\n",
      "\n",
      "Token: Hello\n",
      "Available Tags:  ['B-PRODUCT', 'I-PRODUCT', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE', 'O']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“˜ data_labeling.ipynb or data_labeling.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# ğŸ“¥ Step 1: Load cleaned messages\n",
    "input_path = \"../data/processed/telegram_messages_20250621_052911_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# ğŸ§ª Step 2: Sample 50 tokenized messages\n",
    "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_sample = df_shuffled.head(50)\n",
    "\n",
    "unlabeled_data = [{\"tokens\": eval(row[\"tokens\"])} for _, row in df_sample.iterrows()]\n",
    "\n",
    "# ğŸ‘€ Step 3: Show 5 for manual labeling\n",
    "print(\"\\nğŸ” Preview of messages for labeling:\\n\")\n",
    "for i, sample in enumerate(unlabeled_data[:5]):\n",
    "    print(f\"ğŸŸ¦ Sample {i+1}:\")\n",
    "    print(\"Tokens:\", sample[\"tokens\"])\n",
    "    print()\n",
    "\n",
    "# âœï¸ Step 4: Add your labels manually here\n",
    "# For each labeled example, ensure:\n",
    "#   - token list length == label list length\n",
    "#   - BIO labels use the format B-TAG, I-TAG, O\n",
    "\n",
    "labeled_data = [\n",
    "    {\n",
    "        \"tokens\": [\"áˆˆáˆáŒ†á‰½\", \"áŒ«áˆ›\", \"á‰ \", \"350\", \"á‰¥áˆ­\"],\n",
    "        \"labels\": [\"B-PRODUCT\", \"I-PRODUCT\", \"O\", \"B-PRICE\", \"I-PRICE\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"á‰ áŠ á‹²áˆµ\", \"áŠ á‰ á‰£\", \"á‹¨áˆšáŒˆáŠ\", \"áˆ˜áŠªáŠ“\"],\n",
    "        \"labels\": [\"B-LOC\", \"I-LOC\", \"O\", \"B-PRODUCT\"]\n",
    "    }\n",
    "    # ğŸ” Copy more token sets from `unlabeled_data` and annotate them\n",
    "]\n",
    "\n",
    "# ğŸ’¾ Step 5: Save to CoNLL format\n",
    "def save_conll(data, filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            for token, label in zip(item[\"tokens\"], item[\"labels\"]):\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"âœ… Saved {len(data)} labeled samples to: {filepath}\")\n",
    "\n",
    "# ğŸ’½ Step 6: Export annotated data\n",
    "save_conll(labeled_data, \"data/labeled/amharic_ner.conll\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ“¦ Imports\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data/unlabeled\", exist_ok=True)\n",
    "\n",
    "\n",
    "# ğŸ“¥ Load tokenized unlabeled data\n",
    "df = pd.read_pickle(\"data/unlabeled/unlabeled_data.pkl\")  # Or replace with your variable\n",
    "\n",
    "# ğŸ”¢ Number of samples to label\n",
    "NUM_TO_LABEL = 5  # You can increase as needed\n",
    "\n",
    "# ğŸ“ Define available tags\n",
    "label_options = [\"B-PRODUCT\", \"I-PRODUCT\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\", \"O\"]\n",
    "\n",
    "# ğŸ§  Initialize labeled data container\n",
    "labeled_data = []\n",
    "\n",
    "# ğŸ§° Labeling function\n",
    "def label_sample(tokens):\n",
    "    print(f\"\\nğŸŸ¨ Tokens to label: {' '.join(tokens)}\\n\")\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        print(f\"Token: {token}\")\n",
    "        print(\"Available Tags: \", label_options)\n",
    "        label = input(\"Enter label: \").strip().upper()\n",
    "        while label not in label_options:\n",
    "            label = input(\"Invalid label. Try again: \").strip().upper()\n",
    "        labels.append(label)\n",
    "    return {\"tokens\": tokens, \"labels\": labels}\n",
    "\n",
    "# â–¶ï¸ Start labeling\n",
    "for i in range(NUM_TO_LABEL):\n",
    "    print(f\"\\nğŸ”· Sample {i + 1}/{NUM_TO_LABEL}\")\n",
    "    tokens = df.iloc[i][\"tokens\"]\n",
    "    labeled_sample = label_sample(tokens)\n",
    "    labeled_data.append(labeled_sample)\n",
    "\n",
    "# ğŸ’¾ Save to CoNLL format\n",
    "def save_conll(data, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            for token, label in zip(item[\"tokens\"], item[\"labels\"]):\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_conll(labeled_data, \"data/labeled/amharic_ner.conll\")\n",
    "print(\"âœ… Done labeling. Saved to: data/labeled/amharic_ner.conll\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Sample 1:\n",
      "Tokens: ['One', 'Step', 'Hair', 'Dryer', 'Styler', 'áŠ¨áˆ­áˆ', 'áˆˆáˆ˜áˆµáˆ«á‰µ', 'áˆˆáˆ›áˆˆáˆµáˆˆáˆµ', 'áŠ¥áŠ•á‹²áˆáˆ', 'áˆˆáˆ›á‹µáˆ¨á‰…', 'á‹¨áˆšá‹«áŒˆáˆˆáŒáˆ']\n",
      "\n",
      "ğŸŸ¦ Sample 2:\n",
      "Tokens: ['áˆ›áˆµáˆáŠ•áŒ áˆªá‹«á‹áŠ•', 'á‰°áŒ­áŠá‹', 'áŠ áˆáŠ‘áŠ‘', 'á‹­áˆ˜á‹áŒˆá‰¡', 'á¤', '10', 'á‰…áŠ“áˆ½', 'á‹«áŒáŠ™', '!', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example tokenized samples (replace this with your actual data)\n",
    "samples = [\n",
    "    ['One', 'Step', 'Hair', 'Dryer', 'Styler', 'áŠ¨áˆ­áˆ', 'áˆˆáˆ˜áˆµáˆ«á‰µ', 'áˆˆáˆ›áˆˆáˆµáˆˆáˆµ', 'áŠ¥áŠ•á‹²áˆáˆ', 'áˆˆáˆ›á‹µáˆ¨á‰…', 'á‹¨áˆšá‹«áŒˆáˆˆáŒáˆ'],\n",
    "    ['áˆ›áˆµáˆáŠ•áŒ áˆªá‹«á‹áŠ•', 'á‰°áŒ­áŠá‹', 'áŠ áˆáŠ‘áŠ‘', 'á‹­áˆ˜á‹áŒˆá‰¡', 'á¤', '10', 'á‰…áŠ“áˆ½', 'á‹«áŒáŠ™', '!', '!'],\n",
    "    # Add more samples as needed\n",
    "]\n",
    "\n",
    "# Print each sample in markdown style for labeling\n",
    "for i, tokens in enumerate(samples, start=1):\n",
    "    print(f\"ğŸŸ¦ Sample {i}:\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print()  # Blank line for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ¯', 'Three-layer', 'Baby', 'Milk', 'Powder', 'Container', 'ğŸ’¯', 'High', 'Quality', 'ğŸ‘', 'Three', 'Layer', 'No-Spill', 'Baby', 'Feeding', 'Milk', 'Powder', 'Food', 'Dispenser.', 'A', 'perfect', 'storage', 'for', 'travel', 'or', 'home', 'use.', 'ğŸ‘áŠ¥áŠ“á‰µ', 'áˆáŒ‡áŠ•', 'á‹­á‹›', 'á‹¨á‰°áˆˆá‹«á‹¨', 'á‰¦á‰³', 'áˆµá‰µáŠ•á‰€áˆ³á‰€áˆµ', 'á‹¨á‹±á‰„á‰µ', 'á‹ˆá‰°á‰µ', 'á‹¨áˆ˜áˆ³áˆ°áˆ‰á‰µáŠ•', 'áŠ áˆµáˆáˆ‹áŒŠ', 'á‹¨áˆáŒ†á‰½', 'áˆáŒá‰¥', 'á‹­á‹', 'áˆˆáˆ˜áŠ•á‰€áˆ³á‰€áˆµ', 'á‹¨áˆšáˆ¨á‹³', '3', 'á“áˆ­á‰²áˆ½áŠ•', 'á‹«áˆˆá‹', 'áŠ áˆªá', 'áŠ®áŠ•á‰´áŠáˆ­', 'á‹‹áŒ‹á¦', '500á‰¥áˆ­']\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"ğŸ¯ Three-layer Baby Milk Powder Container \n",
    "ğŸ’¯ High Quality \n",
    "\n",
    "ğŸ‘ Three Layer No-Spill Baby Feeding Milk Powder Food Dispenser. A perfect storage for travel or home use.\n",
    "\n",
    "ğŸ‘áŠ¥áŠ“á‰µ áˆáŒ‡áŠ• á‹­á‹› á‹¨á‰°áˆˆá‹«á‹¨ á‰¦á‰³ áˆµá‰µáŠ•á‰€áˆ³á‰€áˆµ\n",
    "á‹¨á‹±á‰„á‰µ á‹ˆá‰°á‰µ á‹¨áˆ˜áˆ³áˆ°áˆ‰á‰µáŠ• áŠ áˆµáˆáˆ‹áŒŠ á‹¨áˆáŒ†á‰½ áˆáŒá‰¥ á‹­á‹ áˆˆáˆ˜áŠ•á‰€áˆ³á‰€áˆµ á‹¨áˆšáˆ¨á‹³ 3 á“áˆ­á‰²áˆ½áŠ• á‹«áˆˆá‹ áŠ áˆªá áŠ®áŠ•á‰´áŠáˆ­\n",
    "\n",
    "á‹‹áŒ‹á¦ 500á‰¥áˆ­\"\"\"\n",
    "print(sample.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of product samples: 2\n",
      "Samples found:\n",
      "- ğŸ“Œ Only baby 3in1 double bottle milk warmer,sterilizer,food steamer\n",
      "- ğŸ“Œ Mini Pocket UV Umbrella\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ads_text = \"\"\"\n",
    "ğŸ“Œ Only baby 3in1 double bottle milk warmer,sterilizer,food steamer\n",
    "\n",
    "âš¡ï¸áˆˆáˆ•áƒáŠ• á‹ˆá‰°á‰µ áˆ›áˆá‰‚á‹«\n",
    "âš ï¸ á‰ á‰°áŒ¨áˆ›áˆª áˆáŒá‰¥ áˆˆáˆ˜á‰€á‰€áˆ á‹¨áˆšáˆ†áŠ• \n",
    "\n",
    "á‹‹áŒ‹á¦Â  ğŸ’²ğŸ·Â 3000Â  á‰¥áˆ­\n",
    "\n",
    "â™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ğŸ”¥ğŸ”¥ğŸ”¥\n",
    "\n",
    "ğŸ¢ áŠ á‹µáˆ«áˆ»ğŸ‘‰\n",
    "\n",
    "ğŸ“â™¦ï¸#áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰… á‰¢áˆ® á‰. S05/S06\n",
    "\n",
    "Â Â Â Â  ğŸ’§ğŸ’§ğŸ’§ğŸ’§\n",
    "\n",
    "Â Â Â  ğŸ“² 0902660722\n",
    "Â Â Â  ğŸ“² 0928460606 \n",
    "\n",
    "ğŸ”–\n",
    "ğŸ’¬á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™ğŸ”½\n",
    "\n",
    "@zemencallcenter \n",
    "@zemenexpressadmin\n",
    "\n",
    "áˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\n",
    "https://telegram.me/zemenexpress\n",
    "\n",
    "ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥\n",
    "\n",
    "ğŸ“Œ Mini Pocket UV Umbrella\n",
    "\n",
    "ğŸ‘ á‹¨á‰€áˆˆáˆ áŠ áˆ›áˆ«áŒ­ áŠ áˆ‹á‰¸á‹\n",
    "\n",
    "â˜”á‰ áŒ£áˆ á‰€áˆ‹áˆá£ áˆˆáˆ˜á‹«á‹ áˆá‰¹ áŒ¥áˆ‹ â˜”ï¸\n",
    "â˜”á‰ á‰µáŠ•áˆ½ á‹¨áŠ¥áŒ… á‰¦áˆ­áˆ³ á‹ˆá‹­áˆ á‰ áŠªáˆµ áˆ˜á‹«á‹ á‹¨áˆšá‰½áˆâ˜”\n",
    "\n",
    "#Specifications: \n",
    "ğŸ‘Compact & Light-Weight\n",
    "ğŸ‘Unique Design\n",
    "ğŸ‘UV protection\n",
    "\n",
    "Â á‹‹áŒ‹á¦Â  ğŸ’µğŸ·Â  1000á‰¥áˆ­\n",
    "\n",
    "â™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹ ğŸ”¥ğŸ”¥ğŸ”¥\n",
    "\n",
    "ğŸ¢ áŠ á‹µáˆ«áˆ»ğŸ‘‰\n",
    "\n",
    "ğŸ“â™¦ï¸#áˆ˜áŒˆáŠ“áŠ›_áˆ˜áˆ°áˆ¨á‰µ_á‹°á‹áˆ­_áˆáˆ_áˆáˆˆá‰°áŠ›_áá‰… á‰¢áˆ® á‰. S05/S06\n",
    "\n",
    "Â Â Â Â  ğŸ’§ğŸ’§ğŸ’§ğŸ’§\n",
    "\n",
    "Â Â Â  ğŸ“² 0902660722\n",
    "Â Â Â  ğŸ“² 0928460606 \n",
    "\n",
    "ğŸ”–\n",
    "ğŸ’¬á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™ğŸ”½\n",
    "\n",
    "@zemencallcenter \n",
    "@zemenexpressadmin\n",
    "\n",
    "áˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\n",
    "https://telegram.me/zemenexpress\n",
    "\"\"\"\n",
    "\n",
    "# Find all lines starting with ğŸ“Œ (likely product titles)\n",
    "product_clues = re.findall(r\"ğŸ“Œ.*?(?=\\n)\", ads_text)\n",
    "\n",
    "print(\"Number of product samples:\", len(product_clues))\n",
    "print(\"Samples found:\")\n",
    "for clue in product_clues:\n",
    "    print(\"-\", clue)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
